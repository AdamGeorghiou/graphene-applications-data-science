{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection...\n",
      "Searching for: graphene applications\n",
      "Error: API returned status code 429\n",
      "Response: {\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
      "\n",
      "Saving data...\n",
      "No data to save!\n",
      "\n",
      "Analyzing results...\n",
      "Data collection summary: {'status': 'No data collected'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Create necessary directories\n",
    "DATA_DIR = '/Users/adamgeorghiou/Desktop/GIM/Project/data/raw/semantic_scholar'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='data_collection.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class GrapheneResearchCollector:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def search_papers(self, num_results=100):\n",
    "        \"\"\"\n",
    "        Search Semantic Scholar for graphene-related papers\n",
    "        \"\"\"\n",
    "        query = \"graphene applications\"\n",
    "        print(f\"Searching for: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Parameters for the API request\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'limit': min(num_results, 100),  # API limit is 100 per request\n",
    "                'fields': 'title,abstract,year,citationCount,authors,url'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                self.base_url,\n",
    "                params=params,\n",
    "                headers=self.headers\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                papers = response.json().get('data', [])\n",
    "                print(f\"Found {len(papers)} papers\")\n",
    "                \n",
    "                for paper in papers:\n",
    "                    try:\n",
    "                        # Extract author names\n",
    "                        authors = [author.get('name', '') for author in paper.get('authors', [])]\n",
    "                        \n",
    "                        paper_data = {\n",
    "                            'title': paper.get('title', 'No title'),\n",
    "                            'authors': ', '.join(authors),\n",
    "                            'year': paper.get('year'),\n",
    "                            'abstract': paper.get('abstract', 'No abstract'),\n",
    "                            'citations': paper.get('citationCount', 0),\n",
    "                            'url': paper.get('url', 'No URL'),\n",
    "                            'collection_date': datetime.now().isoformat(),\n",
    "                            'source': 'Semantic Scholar'\n",
    "                        }\n",
    "                        \n",
    "                        self.data.append(paper_data)\n",
    "                        print(f\"Successfully collected paper: {paper_data['title'][:100]}...\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing paper: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "            else:\n",
    "                print(f\"Error: API returned status code {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in search: {str(e)}\")\n",
    "            logging.error(f\"Error in search: {str(e)}\")\n",
    "            \n",
    "    def save_data(self, filename='graphene_papers.csv'):\n",
    "        \"\"\"\n",
    "        Save collected data to CSV\n",
    "        \"\"\"\n",
    "        if not self.data:\n",
    "            print(\"No data to save!\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.data)\n",
    "            output_path = os.path.join(DATA_DIR, filename)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Successfully saved {len(self.data)} papers to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {str(e)}\")\n",
    "        \n",
    "    def analyze_initial_data(self):\n",
    "        \"\"\"\n",
    "        Basic analysis of collected data\n",
    "        \"\"\"\n",
    "        if not self.data:\n",
    "            return {\"status\": \"No data collected\"}\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.data)\n",
    "            summary = {\n",
    "                'total_papers': len(df),\n",
    "                'citations_available': df['citations'].notna().sum(),\n",
    "                'total_citations': df['citations'].fillna(0).sum(),\n",
    "                'years_available': df['year'].notna().sum()\n",
    "            }\n",
    "            \n",
    "            # Add year range if available\n",
    "            years = df['year'].dropna()\n",
    "            if not years.empty:\n",
    "                summary['year_range'] = f\"{int(years.min())} - {int(years.max())}\"\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "def main():\n",
    "    collector = GrapheneResearchCollector()\n",
    "    \n",
    "    print(\"Starting data collection...\")\n",
    "    collector.search_papers(num_results=10)  # Start with just 10 papers for testing\n",
    "    \n",
    "    print(\"\\nSaving data...\")\n",
    "    collector.save_data()\n",
    "    \n",
    "    print(\"\\nAnalyzing results...\")\n",
    "    summary = collector.analyze_initial_data()\n",
    "    print(\"Data collection summary:\", summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
